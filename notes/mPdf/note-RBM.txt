RBM - A stachastic artificial neural network that can learn a probability distribution over its set of inputs. 
Network can be trained in either supervised or unsepervised ways, depending on the task
RBM neurons must form a bipartite graph.

Energy function: -a.T v - b.T h - v.T W h
	where h - hidden units,
		  v - visible units,
		  a - bias weigths for visible units
		  b - bias weights for hidden units

Probability distribution is defined in the terms of the energy function: P(v, h) = 1/Z e^(-E(v, h))
	where Z - the sum of e^(-E(v,h)) over all possible configuration (in other words, just normalized constant
			  to ensure the probability distribution sums to 1)
    
Marginal probability of visible vector of booleans is the sum over all possible hidden layer configuration:
	P(v) = = 1/Z e^(-E(v, h))

Since the RMB has the shape of a bipartite graph, with no intra-layer connections, the hidden units activations are mutually
independent given the visible unit activations and conversly. That is, for m visible units and for n visible units, 
the conditional probability of a configuration of the visible units v, given a configuration of the the hidden units h, is:
	P(v | h) = product_over P(v_i | h)
	
	and conversely, the conditional probability of h given v is:
	
	P(h | v) = product_over P(h_i | v)
	
	
	