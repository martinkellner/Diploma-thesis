
Energy based model

	-- ucenie zodpoveda v modifikacii energy funkcie tak, ze jej tvar ma ziaduje vlastnosti
	-- Energy based pravdepodobnostne modely definuju pravdepodobnostu distribuciu cez ako:
	
			p(x) = e^-E(x)/Z, kde Z je normalizacny faktor: Z = sum e^{E(x)}
			
			Moze byt ucene pomocou stochastickeho gradient descent na empicka negativna log-vierihodnosti
			na trerningovych datach.
			
	-- v mnohych pripadoch nesledujeme len x, alebo chcem zahrnut nejaku nepozorovanu premenu nba 
		zvysenie sily momentu:
			
			P(x) = sum P(x,h) = sum e^{-E(x,h)}/Z

RBM: 

	-- jedna skryta vstva 
	-- jedna viditelna
	
	-- bipartitny graf 
	
	
	Energy function: 
		
		E(v, h) = -b'v - c'v - h'Wv,
			
			kde W reprezentuje vahy na spojeniach a b,c su offset(bias?) pre viz/hid layer
		
		Potom pre volnu energiu:
			
			F(v) = -b'v - sum log sum e^{h_i(c_i + W_i v)}
		
		Vdaka specialnej strukture RBM, viditelne a skryte jednoty nezavisle od seba mozeme napisat:
		
			p(h|v) = prod p(h_i|v)
			p(v|h) = prod p(v_j|h)
		
Samplovanie v RBM:
	
	Metoda, ktora aproximuje sekvencie pozorovani z join distribution viacerych premmenych
	
	Sample p(x) ziskame zbenutim markovho retazca az do convergencie a pouzitim Gibbs sampling ako prechodovy operator.
	
	Gibbs sampling pre spojenie N premennych:

		S = (S1, S2, ..., Sn) sa vytvory pomocou postupnosti krokov vo forme:
			
			S_i = p(S_i|S_{-i}), kde S_{-i} obsahuje N - 1 inych nahodnych premennych
			okrem S_i
			
		Pre RBm S pozostava zo skupiny sktypich a viditelnych jednotiek.
		
		Viditelne neurony su samplovane cez hodnoty fixovanych hodnot sktytych neuronov.
		Skryte analogicky
		
			h(n+1) ~ sigm(W'v^(n) +c)
			v(n+1) ~ sigm(Wh^(n+1) +b)
			
			kde h(n) odkazuje na mnozinu skrytych jednotie na n-krokoch markovoho retazca.
			Napriklad h_i(n+1) je nahodne vybrane medzi 0 a 1 s pravdepodonostou sigm(W_j'v(n) + c_i) alogicky viditenlne
		
		Pri case -> nekonecno, samples(v_t, h_t) konverguju do presnej hodnoty samplu p(v,h)
		
	